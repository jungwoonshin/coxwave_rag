import json
import logging
from typing import Any, Dict, List

# Import components from your system
from utils.prompt import PromptBuilder

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

class FailureAnalyzer:
    """
    Class for analyzing RAG system failures.
    """
    def __init__(self, llm_model: Any, timeout: int = 30):
        """
        Initialize the failure analyzer.
        
        Args:
            llm_model: The LLM to use for analysis
            timeout: Timeout in seconds for LLM generation
        """
        self.llm_model = llm_model
        self.timeout = timeout
    
    def analyze(self, question: str, retrieved_docs: List[Dict], correct_answer: str, 
              generated_answer: str, evaluation: Dict) -> Dict:
        """
        Analyze why the system failed to answer correctly.
        
        Args:
            question: The test question
            retrieved_docs: The documents retrieved by the RAG system
            correct_answer: The correct answer from the FAQ
            generated_answer: The answer generated by the LLM
            evaluation: The evaluation results
            
        Returns:
            Dictionary with failure analysis
        """
        # Check if correct answer was in retrieved documents
        correct_in_retrieved = False
        retrieval_rank = -1
        
        for i, doc in enumerate(retrieved_docs):
            if doc["answer"] == correct_answer:
                correct_in_retrieved = True
                retrieval_rank = i
                break
        
        # Format retrieved documents for the prompt
        retrieved_context = ""
        for i, doc in enumerate(retrieved_docs):
            retrieved_context += f"Document {i+1}:\n"
            retrieved_context += f"Question: {doc['question']}\n"
            retrieved_context += f"Answer: {doc['answer']}\n"
            retrieved_context += f"Score: {doc['score']}\n\n"
        
        # Format the evaluation results
        eval_details = [
            f"Semantic Similarity (Correctness): {evaluation['correctness']}",
            f"Completeness: {evaluation['completeness']}",
            f"Word Overlap (Relevance): {evaluation['relevance']}",
            f"Conciseness: {evaluation['conciseness']}",
            f"Overall: {evaluation['overall']}",
            f"Passed: {evaluation['passed']}"
        ]
        
        # Add specific embedding metrics if available
        if 'cosine_similarity' in evaluation:
            eval_details.append(f"Cosine Similarity: {evaluation['cosine_similarity']}")
        if 'length_ratio' in evaluation:
            eval_details.append(f"Length Ratio: {evaluation['length_ratio']}")
        if 'word_overlap' in evaluation:
            eval_details.append(f"Word Overlap Score: {evaluation['word_overlap']}")
        if 'reasons' in evaluation:
            eval_details.append(f"Failure Reasons: {evaluation['reasons']}")
        
        # Get the failure analysis prompt
        analysis_prompt = PromptBuilder.get_failure_analysis_prompt().format(
            question=question,
            retrieved_context=retrieved_context,
            correct_answer=correct_answer,
            generated_answer=generated_answer,
            evaluation_details="\n".join(eval_details),
            correct_in_retrieved="Yes" if correct_in_retrieved else "No",
            retrieval_rank=retrieval_rank if retrieval_rank >= 0 else "N/A"
        )
        
        # Create a basic fallback analysis in case of errors
        fallback_analysis = {
            "primary_failure_mode": "Retrieval Failure" if not correct_in_retrieved else "Context Utilization Failure",
            "explanation": "Unable to generate detailed analysis.",
            "suggestions": "Improve the retrieval system to better match questions to answers."
        }
        
        try:
            # Generate the analysis with timeout protection
            response = self._generate_with_timeout(
                prompt=analysis_prompt,
                temperature=0.3,
                max_tokens=1024
            )
            
            # Parse the JSON response
            try:
                analysis = json.loads(response)
                
                # Validate analysis format
                required_keys = ["primary_failure_mode", "explanation", "suggestions"]
                if not all(key in analysis for key in required_keys):
                    missing_keys = [key for key in required_keys if key not in analysis]
                    logger.warning(f"Analysis response missing required keys: {missing_keys}")
                    for key in missing_keys:
                        analysis[key] = fallback_analysis[key]
                
            except json.JSONDecodeError as e:
                logger.warning(f"Failed to parse analysis response as JSON: {e}. Raw response: {response[:100]}...")
                analysis = fallback_analysis
            
            # Add retrieval stats
            analysis["correct_in_retrieved"] = correct_in_retrieved
            analysis["retrieval_rank"] = retrieval_rank if retrieval_rank >= 0 else None
            
            # Add embedding-specific metrics for more detailed analysis
            if 'cosine_similarity' in evaluation:
                analysis["semantic_similarity"] = evaluation['cosine_similarity']
            if 'word_overlap' in evaluation:
                analysis["word_overlap"] = evaluation['word_overlap']
            
            return analysis
            
        except Exception as e:
            logger.error(f"Error analyzing failure: {e}")
            fallback_analysis.update({
                "error": str(e),
                "correct_in_retrieved": correct_in_retrieved,
                "retrieval_rank": retrieval_rank if retrieval_rank >= 0 else None
            })
            return fallback_analysis
    
    def _generate_with_timeout(self, prompt: str, temperature: float = 0.3, max_tokens: int = 1024) -> str:
        """
        Generate text with a timeout to prevent hanging.
        
        Args:
            prompt: The prompt to send to the model
            temperature: The sampling temperature
            max_tokens: Maximum tokens to generate
            
        Returns:
            The generated text as a string
        
        Raises:
            TimeoutError: If generation takes longer than self.timeout
        """
        import concurrent.futures
        import threading
        
        def generate_text():
            nonlocal result
            try:
                response = self.llm_model.generate(
                    prompt=prompt,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    stream=False
                )
                
                # Convert generator to string if needed
                if hasattr(response, '__iter__') and not isinstance(response, (str, bytes, bytearray)):
                    response_text = ""
                    for chunk in response:
                        response_text += chunk
                    response = response_text
                
                result = response
            except Exception as e:
                result = f"{{\"primary_failure_mode\": \"Analysis Error\", \"explanation\": \"Error during generation: {str(e)}\", \"suggestions\": \"Check system configuration.\"}}"
        
        result = None
        thread = threading.Thread(target=generate_text)
        thread.daemon = True
        thread.start()
        thread.join(self.timeout)
        
        if thread.is_alive():
            # The thread is still running, meaning we hit the timeout
            logger.warning(f"LLM generation timed out after {self.timeout} seconds")
            return "{\"primary_failure_mode\": \"Timeout Error\", \"explanation\": \"Analysis generation timed out.\", \"suggestions\": \"Consider increasing the timeout or using a faster model.\"}"
        
        if result is None:
            # Something went wrong but we didn't get an exception
            return "{\"primary_failure_mode\": \"Unknown Error\", \"explanation\": \"Analysis failed without error message.\", \"suggestions\": \"Check logs for details.\"}"
        
        return result
    
    def analyze_batch(self, questions: List[str], retrieved_docs_list: List[List[Dict]], 
                    correct_answers: List[str], generated_answers: List[str], 
                    evaluations: List[Dict], max_retries: int = 2) -> List[Dict]:
        """
        Batch analysis of failures for multiple test cases with retry logic.
        
        Args:
            questions: List of test questions
            retrieved_docs_list: List of retrieved documents for each question
            correct_answers: List of correct answers
            generated_answers: List of generated answers
            evaluations: List of evaluation results
            max_retries: Maximum number of retry attempts for failed analyses
            
        Returns:
            List of dictionaries with failure analyses
        """
        # Validate input
        if not (len(questions) == len(retrieved_docs_list) == len(correct_answers) == 
                len(generated_answers) == len(evaluations)):
            raise ValueError("All input lists must have the same length")
        
        # Identify which cases failed
        failed_indices = [i for i, evaluation in enumerate(evaluations) if not evaluation["passed"]]
        
        # If no failures, return empty list of analyses
        if not failed_indices:
            return [None] * len(questions)
        
        # Process each failure individually
        analyses = [None] * len(questions)
        
        for i in failed_indices:
            retry_count = 0
            success = False
            
            while not success and retry_count <= max_retries:
                try:
                    if retry_count > 0:
                        logger.info(f"Retry attempt {retry_count} for analyzing failure at index {i}")
                    
                    analysis = self.analyze(
                        question=questions[i],
                        retrieved_docs=retrieved_docs_list[i],
                        correct_answer=correct_answers[i],
                        generated_answer=generated_answers[i],
                        evaluation=evaluations[i]
                    )
                    
                    analyses[i] = analysis
                    success = True
                    
                except Exception as e:
                    logger.error(f"Error analyzing failure at index {i} (attempt {retry_count+1}): {e}")
                    retry_count += 1
                    
                    # If we've exhausted retries, create a fallback analysis
                    if retry_count > max_retries:
                        analyses[i] = {
                            "primary_failure_mode": "Analysis Error",
                            "explanation": f"Error during failure analysis after {max_retries+1} attempts: {str(e)}",
                            "suggestions": "Check system logs for error details.",
                            "error": str(e)
                        }
        
        return analyses
    
    def analyze_with_rule_based_fallback(self, question: str, retrieved_docs: List[Dict], 
                                         correct_answer: str, generated_answer: str, 
                                         evaluation: Dict) -> Dict:
        """
        Analyze failure with rule-based fallback if LLM analysis fails.
        
        Args:
            question: The test question
            retrieved_docs: Documents retrieved by the RAG system
            correct_answer: The correct answer
            generated_answer: The answer generated by the LLM
            evaluation: The evaluation results
            
        Returns:
            Dictionary with failure analysis
        """
        try:
            # First try LLM-based analysis
            return self.analyze(question, retrieved_docs, correct_answer, generated_answer, evaluation)
        except Exception as e:
            logger.warning(f"LLM analysis failed: {e}, falling back to rule-based analysis")
            
            # Check if correct answer was in retrieved documents
            correct_in_retrieved = False
            retrieval_rank = -1
            
            for i, doc in enumerate(retrieved_docs):
                if doc["answer"] == correct_answer:
                    correct_in_retrieved = True
                    retrieval_rank = i
                    break
            
            # Rule-based analysis
            if not correct_in_retrieved:
                failure_mode = "Retrieval Failure"
                explanation = "The system failed to retrieve the document with the correct answer."
                suggestions = "Improve embedding quality or retrieval algorithm."
            elif retrieval_rank > 0:
                failure_mode = "Relevance Failure"
                explanation = f"The correct document was retrieved but ranked {retrieval_rank+1}."
                suggestions = "Improve document ranking algorithm."
            elif evaluation['correctness'] < 3.0:
                failure_mode = "Semantic Error"
                explanation = "The generated answer has low semantic similarity to the correct answer."
                suggestions = "Adjust prompt to encourage more accurate responses."
            elif evaluation['completeness'] < 3.0:
                failure_mode = "Completeness Error"
                explanation = "The generated answer is missing important information from the correct answer."
                suggestions = "Adjust prompt to encourage more complete responses."
            else:
                failure_mode = "Context Utilization Failure"
                explanation = "The model didn't effectively use the retrieved context."
                suggestions = "Improve prompt engineering to help model better utilize context."
            
            return {
                "primary_failure_mode": failure_mode,
                "explanation": explanation,
                "suggestions": suggestions,
                "correct_in_retrieved": correct_in_retrieved,
                "retrieval_rank": retrieval_rank if retrieval_rank >= 0 else None,
                "analysis_method": "rule-based",
                "llm_analysis_error": str(e)
            }
